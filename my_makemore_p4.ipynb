{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa70589",
   "metadata": {},
   "source": [
    "# Becoming a backprop ninja\n",
    "\n",
    "This is for the 'part 4 - becoming a backprop ninja' lecture. He taked issue w/ us (i think he's feeling 'blindly') calling 'loss.backward' and so using PyTorch's autograd functionality to get our weights. He thinks it's important and useful for us to understand what's going on, as he writes about in https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b. \n",
    "\n",
    "We did do micrograd already, but micrograd only thinks about scalars. This lecture is about tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7f027",
   "metadata": {},
   "source": [
    "Historically, interesting to know that back in just 2012, people wrote their backward pass by hand (or used other algorithms than backprop entirely), while now everyone just calls loss.backward(), and 'we've lost something'. They'd use Matlab! (Since it had a convenient tensor class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517d6d06",
   "metadata": {},
   "source": [
    "# Overall plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da128b",
   "metadata": {},
   "source": [
    "We'll do the same multilayer perceptron network, and same training loop, but we'll do the backprop by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce164909",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187cc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d9fd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007d8577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebb5cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cff848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length, how many chars to predict next char?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        # training row for each set of block_size chars (with '.' at end)\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch] # 'next' char, after context\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # new context shifts right and adds prev 'next' char\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words)) # 80% for train\n",
    "n2 = int(0.9*len(words)) # 10% for dev/validation and test\n",
    "\n",
    "Xtr,  Ytr = build_dataset(words[:n1])    # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # 10%\n",
    "Xte,  Yte = build_dataset(words[n2:])    # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adb1304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa3f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to use later to compare manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d568e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # dimensionality of the character vectors\n",
    "n_hidden = 64 # neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g)\n",
    "# Layer one\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # doesn't matter I think because of batch norm\n",
    "# Layer two\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm params\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# note: many of the params are initialized in non-standard ways beacuse\n",
    "# sometimes initializing with e.g. all zeros could mask an incorrect\n",
    "# impl of the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4beb414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
